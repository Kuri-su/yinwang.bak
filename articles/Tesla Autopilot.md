<div class="inner">
<h2>Tesla Autopilot</h2>
<blockquote>
<p>以下内容是《<a href="http://www.yinwang.org/blog-cn/2015/12/12/tesla-model-s">Tesla Model S的设计失误</a>》一文中新加入的小节。由于写作时间相距太远，而且由于它的<a href="http://www.reuters.com/article/us-tesla-autopilot-idUSKCN0UO0NM20160110">时效性</a>，现在也把它单独提出来，独立成文。</p>
</blockquote>
<p>两个月前，Tesla 通过“软件更新”，使 Model S 具有了初级的“自动驾驶”（autopilot）功能。这个功能可以让 Model S 自动地，沿着有“清晰边界线”的车道行驶，根据前后车辆的速度相应的加速和减速。</p>
<p>这貌似一个很新很酷的功能，咋一看跟 Google 的自动车有的一拼（其实差得天远）。然而在推出后不久，YouTube 上出现了一些视频（<a href="https://www.youtube.com/watch?v=MrwxEX8qOxA">视频1</a>，<a href="https://www.youtube.com/watch?v=Lx3-epk_ztQ">视频2</a>，<a href="https://www.youtube.com/watch?v=LJnYCEQwtHs">视频3</a>，<a href="https://www.youtube.com/watch?v=rkZ-jhLxrVc">视频4</a>，<a href="https://www.youtube.com/watch?v=mLOG1bw3vSM">视频5</a>）。它们显示，autopilot 在某些情况下有可能进行错误的判断和操作，有些险些造成严重的迎面车祸。</p>
<p><a href="https://www.youtube.com/watch?v=MrwxEX8qOxA">
<img src="http://www.yinwang.org/images/model-s-autopilot-frontal.png" width="80%" />
</a></p>
<p>特别是<a href="https://www.youtube.com/watch?v=MrwxEX8qOxA">视频1</a>显示，在路面线条清晰，天气很好的路上，autopilot 忽然向左，试图转向反方向的车道，差点导致严重的对撞车祸。仔细观察 autopilot 转向之前的情况，是由于路面上有阳光投下来的树影。Autopilot 误以为那是一个障碍物，所以试图把车转上反方向的车道！</p>
<p>从这个简单的视频我们可以看出：</p>
<ol>
<li>
<p>Autopilot 没有对图像进行基本的“阴影消除”，它不能区分阴影和障碍物。阳光强烈，阴影明显的时候，autopilot 可能把阴影当成障碍物。阴影消除在计算机视觉已经研究挺多了，这说明Tesla有可能没有进行基础的计算机视觉研究。缺乏分辨阴影和障碍物的能力，这样的自动驾驶系统是完全不可接受的。</p>
</li>
<li>
<p>道路中间有明显的，表示“禁止超车”的双黄线，对面有来车。Autopilot 为了避开“障碍”，冒着对撞的危险，左转跨越双黄线。这表示 autopilot 连基本的交通规则，紧急情况下的正确操作方式都搞不清楚。或者也许这软件里面连双黄线都没有识别，甚至连这个概念都没有。</p>
<p>对于一个有经验的驾驶员来说，如果发现前方有障碍物，正确的作法不应该是猛烈地转弯避开，而应该是紧急刹车。从视频上我们看出，车子没有刹车减速（保持在 37~38），而是猛烈地左转。而且是等树影到了面前，才忽然进行操作，没有计算提前量。这说明设计 autopilot 的人，连基本的开车常识都不明白。</p>
</li>
</ol>
<p>让我感到悲哀的是，这些视频的很多评论，大部分都在谩骂车主是傻逼：“这是车主自己的责任！”，“Autopilot 只能在高速公路上使用”，“只能在车道上有明确的边界线的时候使用！”，“不能在有很多弯道的地方“，“只能在能够看见前方300米道路的地方使用”，“谁叫你不看说明书的！”…… Elon Musk 也在一次<a href="https://www.youtube.com/watch?v=60-b09XsyqU">采访</a>中明确的告诉记者：“如果用户因为使用 autopilot 而导致了车祸，是用户自己的责任！” 他反复地声明：“autopilot 还处于 beta 版本……” 意思是，你们小心着用！</p>
<p>我对这些说法持不同的观点。首先，Tesla 根本就不应该把一个处于”beta 状态”的功能，自动推送到所有 Model S 的系统里面。实际上，像 autopilot 这种功能，关系到人的生命安全，根本就不应该有”beta版本”或者“测试版本”之说。Tesla 把这样不成熟的系统，强制推送给用户，然后又说如果出了事故，用户负所有责任，这是一种推卸责任的做法。要知道，没有任何人愿意拿自己的生命给 Tesla 做“beta 测试”。</p>
<p>另外，就算是用户没有仔细阅读 autopilot 的使用说明，在“不该”用它的地方（比如路面线条不清晰的地方）使用了autopilot，如果出了车祸，Tesla也应该负完全的责任。理由如下：</p>
<ol>
<li>
<p>作为用户，他们没有义务阅读并且深刻的理解 autopilot 的局限性。在软件行业，存在一种习惯性的“责备用户”的不良风气。如果软件的设计有问题，用户没记住它的毛病，没能有效地绕过，那么如果出了问题，一般被认为是用户的错。Tesla 想把软件行业的这种不正之风，引入到人命关天的汽车行业，那显然是行不通的。</p>
</li>
<li>
<p>Tesla 的 autopilot 实现方式幼稚，局限性实在太多。天气不好的时候不行，路面上的边界线不清晰也不行，光线不好或者有阴影不行，路上有施工的路桩不行，高速出口不行，…… 实际上，在如此苛刻的限定条件下，任何一个汽车厂商都可以做出 Tesla 那种 autopilot。</p>
<p>我自己的便宜 Honda 车，就有偏离车道时发出警告的功能（Lane Drift Warning，LDW）。装个摄像头，来点最简单的图像处理就搞定。在Indiana大学的时候，我们有一门本科级别的课程，就是写代码控制一辆高尔夫球车（也是电动车呢），沿着路面上的线条自动行驶。这根本没什么难度，因为它能正确行驶的条件，实在是太苛刻了。</p>
<p>其它汽车厂商很清楚这种功能的局限性，所以他们没有大肆吹嘘这种“线检测”的技术，或者把它做成autopilot。他们只是把它作为辅助的，提示性的功能。这些汽车厂商理解，作为一个用户，他们不可能，也不应该记住autopilot能正确工作的种种前提条件。</p>
</li>
<li>
<p>用户没有足够的能力来“判断”autopilot正常工作的条件是否满足。比如，路上的线还在，但是被磨损了，颜色很浅，那么autopilot到底能不能用呢？谁也不知道。把判断这些条件是否满足的任务推给用户，就像是在要求用户帮Tesla的工程师debug代码。这显然是不可行的。如果autopilot能够在检测到道路条件不满足的情况下，自动警告用户，并且退出自动驾驶模式，那还稍微合理一些。</p>
</li>
<li>
<p>用户也许没有足够的时间来响应条件的改变。Autopilot自动驾驶的时候，车子有可能最初行驶在较好的条件下（天气好，路面线条清晰），然而随着高速行驶，路面条件有可能急速的变化。有可能上一秒还好好的，下一秒路面线条就不再清晰（<a href="https://www.youtube.com/watch?v=mLOG1bw3vSM">视频5</a>貌似这种情况）。路面条件的变化突如其来，驾驶员没有料到。等他们反应过来，想关闭autopilot的时候，车祸已经发生了。这种情况如果上诉到法庭，稍微明理一点的法官，都应该判Tesla败诉。</p>
</li>
<li>
<p>Autopilot显摆出的“高科技”形象，容易使人产生盲目的信任，以至于疏忽而出现车祸。既然叫做“autopilot”，这意味着它能够不需要人干预，自动驾驶一段时间。既然用户觉得它能自动驾驶，那么他们完全有理由在到达高速路口之前（比如GPS显示还有一个小时才到出口），做一些自己的事情：比如看看手机啊，看看书啊，甚至刷刷牙…… 不然，谁让你叫它是“autopilot”的呢？我坐飞机时，就见过飞行员打开autopilot，上厕所去了。如果启用了autopilot还得一秒钟不停地集中注意力，那恐怕比自己开车还累。自己开车只需要看路，现在有了autopilot，不但要看路，还要盯着方向盘，防止autopilot犯傻出错……</p>
</li>
<li>
<p>Tesla把“beta版”的autopilot推送给所有的Model S，是对社会安全不负责任的做法。你要明白Murphy’s Law：如果一个东西可能出问题，那么就一定会有人让它出问题。Autopilot的功能不成熟，限制条件很多，不容易被正确使用，这不但对Model S的车主自己，而且对其他人也是一种威胁。汽车不是玩具，随便做个新功能，beta版，让人来试用，是会玩出人命的。我觉得Tesla的autopilot，跟无照驾驶的人一样，应该被法律禁止。由于autopilot的复杂性和潜在的危险性，使用autopilot的用户，应该经过DMV考核，在驾照上注明“能正确使用Tesla autopilot”，才准上路。</p>
</li>
<li>
<p>关系到人的生命安全的“免责声明”和“用户协议”，在法律上是无效的。在美国，到处都存在“免责声明”之说。比如你去参加学校组织的春游活动，都要叫你签一个“waiver”，说如果出了安全事故或者意外，你不能把学校告上法庭。这种免责声明，一般在法律上都是无效的。如果由于学校的过错而致使你的身体受了损伤，就算你签了这种waiver，照样可以把学校告上法庭。我估计Tesla的autopilot在启动时，也有这样的免责声明，说如果使用autopolit而出现车祸，Tesla不负责任。由于autopilot直接操控了你的车子，如果真的出了车祸，这跟其它的waiver一样，都是无效的。你照样可以上法庭告他们。</p>
</li>
</ol>
<p>由于意识到这个问题，知道出了问题自己是逃不掉责任的，Tesla最近又通过强制的软件更新，对autopilot的功能进行了一些<a href="http://www.reuters.com/article/us-tesla-autopilot-idUSKCN0UO0NM20160110">限制</a>，说是为了防止用户“滥用”autopilot做一些“疯狂”的事情。Tesla很疯狂，反倒指责用户“滥用”和“疯狂”。这让人很愤慨。</p>
<p>对autopilot进行限制的同时，Tesla又推出了beta版的“<a href="http://www.cnet.com/news/tesla-cars-can-now-self-park-at-your-command">自动趴车</a>”和“召唤”（summon）功能。这些功能貌似很酷，然而它们也附带了许多的限制条件。你只能在某些地方，满足某种特定条件，才能用这些功能。如果你违反这些条件，出了事故，Tesla声称不负责。</p>
<p>这些能够让车子自己移动的功能，跟autopilot一样，同样会给社会带来安全隐患。比如，有人在不该使用自动趴车和summon功能的地方用了它，就可能会导致车祸。这不是用户的问题，而是Tesla根本不应该发布这些不成熟的技术来哗众取巧。</p>
</div>
    