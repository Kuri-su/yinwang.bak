<div class="inner">
<h2>机器与人类智能的差距</h2>
<p>很多人以为“人工智能”就快实现了，是因为他们混淆了“识别”和“理解”。现在所谓的“人工智能”离真正的智能差的实在太远。有多远呢？真正的工作几乎没有开始。</p>
<h3 id="语言">语言</h3>
<p>对于语言，人们常常混淆“语音识别”和“语言理解”。AI 研究者们很多时候还故意把两者混淆起来，所以你经常听他们说：“机器已经能理解人类语言了！” 这些都是胡扯。机器离理解人类语言差距非常远。</p>
<p>现在的机器只能做好“语音识别”，它知道你说的是哪些字，却不能理解你说的什么“意思”。即使加上所谓“知识图谱”，它也不能真的理解，因为知识图谱跟一本同义反义词典差不多，只不过多了一些关系。这些关系全都是文字之间的关系，它停留在文字/语法层面，而没有接触到“语义”。</p>
<p><img src="https://www.yinwang.org/images/knowledge-graph.jpg" width="70%" /></p>
<p>语义根本不是文字，而是更深一层的信息。语义是什么呢？举个例子。如果你听到“猫”这个字，你的脑子里就出现关于猫的很多信息，它们是什么样子，有什么行为，等等。这些是具体形象的信息，它是人对“猫”这个概念的很多经验，这不是文字可以表示的。我把这种信息叫做“常识”。</p>
<p>知识图谱的研究者们试图把词语归一下类，找找其中的关系（IS_A 之类的），以为就能够理解人类语言。可是你们想过人类是如何理解语言的吗？许多人都不知道这些词语之间有什么关系。什么近义词，反义词，IS_A…… 这些学术概念在小孩子的头脑里根本就没有，可是他们却能理解你在说什么，因为他们的脑子里有常识。</p>
<p>所以知识图谱不大可能解决语言理解的问题，它只浮于表面。</p>
<p>要想产生语义，机器必须拥有人的“常识”。常识到底是什么数据，如何获得常识，如何表示，如何利用，谁也不知道。所以理解人类语言是一个毫无头绪的工作，根本不像 AI 人士说的“已经实现了”。</p>
<p>如果不能真的从语义层面理解人类语言，什么“智能客服”，“智能个人助手”，全都只能扯淡。做个玩具忽悠小孩可能还行，真的要用来理解和处理“客服工作”，还是放弃吧。</p>
<h3 id="视觉">视觉</h3>
<p>对于视觉，AI 领域混淆了“图像识别”和“视觉理解”。现在热门的 “AI” 都是“图像识别”，而动物的视觉系统拥有强大的“视觉理解”。视觉理解和图像识别有着本质的不同。</p>
<p>深度学习视觉模型（CNN 一类的）只是从大量数据拟合出从“像素=&gt;名字”的函数。它也许能从一堆像素识别出图中物体的“名字”，但它却不知道那个物体“是什么”，无法对物体进行操作。</p>
<p>“图像识别”跟“语音识别”处于同样的阶段，停留在语法（字面）层面，而没有接触到“语义”。语音识别是“语音=&gt;文字”的转换，而图像识别则是“图像=&gt;文字”的转换。两者都输出文字，而“文字”跟“理解”处于两个不同的层面。文字是表面的符号，你得理解了它才会有意义。</p>
<p>怎样才叫“理解了物体”呢？至少，你得知道它是什么形状的，有哪些组成部分，各部分的位置和边界在哪里，是什么材料做成的，大概有什么性质。这样你才能有效的对它采取行动，达到需要的效果。否则这个物体只是一个方框上面加个标签，不能精确地进行判断和实际的操作。</p>
<p><img src="https://www.yinwang.org/images/ssd-road.jpg" width="70%" /></p>
<p>想想面对各种日常事物的时候，你的脑子里出现的是它们的名字吗？比如你拿起刀准备切水果，旁边没有其他人跟你说话，你的脑子里出现了“刀”这个字吗？没有。你的脑子里出现的不是名字，而是“常识”。常识不是文字，而是一种抽象而具体的数据。</p>
<p>你知道这是一把刀，可是你的头脑提取的不是“刀”这个字，而是刀“是什么”。你的视觉系统告诉你它的结构是什么样的。你知道它是金属做的，你看到刀尖，刀刃，刀把，它也许是折叠的。经验告诉你，刀刃是锋利的可以切东西的地方，碰到可能会受伤，刀把是可以拿的地方。如果刀是折起来的，你得先把它翻开，那么你从哪一头动手才能把它翻开，它的轴在哪里？你的指甲得扣住那个槽，它才不会在翻开的时候打滑……</p>
<p>你顺利拿起刀，开始切水果。可是你的头脑里仍然没有出现“刀”这个字，也没有“刀刃”，“刀把”之类的词。在切水果的同时，你头脑里的“语言中心”可能在哼一首最近很喜欢的歌词，它跟刀没有任何关系。语言只是与其他人沟通的时候需要的工具，自己做事的时候我们并不需要语言。完成切水果的动作，你需要的是由视觉产生的对物体的理解，而不是语言。</p>
<p>你不需要知道一个物品叫什么名字就能正确操作它。如果你的脑子里首先出现的是事物的名字，那么你肯定是很愚钝的人，无法料理自己的生活。现在的“机器视觉”基本就是那样的。机器也许能说出图片上物体的名字，它却不知道它是什么，无法操作它。</p>
<p>这就是我所谓的“视觉理解”与“图像识别”的差别。</p>
<p>如果我们降低标准，只是“识别”物体的名字，那么以像素为基础的图像识别，也是没法像人一样准确识别物体的。人的视觉系统并不是简单的“拍照+识别”，而是“观察+理解+观察+理解+观察+理解……”，是一个动态的，反复的过程。感官接受信息，中间穿插着理解，理解反过来又控制着观察的方向和顺序。</p>
<p>举个例子，假设你从来没见过这个东西，你知道这是什么吗？</p>
<p><img src="https://www.yinwang.org/images/mars-rover.jpg" width="50%" /></p>
<p>一个从来没见过火星车的人，也会知道这是个“车子”。为什么呢？因为它有轮子。为什么你知道那是轮子呢？因为它在地面上，是圆柱形的，外面看上去是橡胶，而且中间有根轴，所以能够沿着轴转动…… 所有的这些分析都是理解，而这些理解都是从经验来的“常识”。</p>
<p>所以一个人从没见过的物体，他也能知道它是什么。没有理解能力的机器是绝对做不到这一点的。</p>
<p>人的眼睛与摄像头有着本质的差异。眼睛是会转动的，它被脑神经控制，敏捷地跟踪着感兴趣的部分：线条，平面，立体结构…… 人的视觉系统能够精确地理解物体的“形状”，理解“拓扑”（topology），而且这些都是 3D 的。人脑看到的不是像素，而是一个 3D 拓扑模型。</p>
<p>眼睛观察的顺序，不是一行一行从上往下把每一个“像素”都记下来，做成 6000x4000 像素的图片，而是聚焦在重点上。它可以沿着直线，也可以沿着弧线观察，可以转着圈，也可以跳来跳去的。所以眼睛采集的信息量可能不大，人脑需要处理的信息也不多。</p>
<p>人能理解点，线，面的概念，理解物体的表面是连续的还是有洞，是凹陷的还是凸起的，分得清里和外，远和近，上下左右…… 他能理解物体的表面是什么质地，如果用手去拿会有什么样的反应。他能想象出物体的背面大概是什么样子，他能在头脑中旋转或者扭曲物体的模型。如果物体中间有缺损，他甚至能猜出那位置之前什么样子。</p>
<p>人的视觉系统比摄像头有趣的多。很多人都看过“光学幻觉”（optical illusion）的图片，它们从一个角度揭示了人的视觉系统背后在做什么。比如下图本来是一个静态的图片，可是你会感觉有很多黑点在白线的交叉处闪烁。</p>
<p><img src="https://www.yinwang.org/images/herman-grid.jpg" width="40%" /></p>
<p>本来是静态图片，你却感觉它在转。</p>
<p><img src="https://www.yinwang.org/images/wheel-rotate.jpg" width="60%" /></p>
<p>这些幻觉说明人的视觉系统不是简单的摄像头一样的东西，特殊功能和机制导致了这些幻觉。这些特殊的机制使得人类视觉不同于机器，使得人能够提取出物体的结构，而不是只看到像素。</p>
<p>提取物体的拓扑结构特征，这就是为什么人可以理解抽象画，漫画，玩具。虽然世界上没有猫和老鼠长那个样子，一个从来没看过《猫和老鼠》动画片的小孩，却知道这是一只猫和一只老鼠，后面有个房子。你试试让一个没有拿《猫和老鼠》剧照训练过的深度学习模型来识别这幅图？</p>
<p><img src="https://www.yinwang.org/images/tom-and-jerry.jpg" width="70%" /></p>
<p>人脑理解“拓扑”的概念，这使得人能够不受具体像素限制而正确处理各种物体。理解穿插在了识别物体的过程中，“观察/理解”成为不可分割的整体。人看到物体的一部分，理解了那是什么，然后继续观察它周围是什么。理解使得人对物体的识别非常准确，甚至可以在信息不完整，模糊，扭曲的情况下工作，在恶劣的天气环境下，有反光的情况下也能识别物体。</p>
<p>说到“反光”，你有想过机器要如何才会知道场景里有一面镜子或者玻璃吗？这是很现实的问题。你的自动车或者机器人要如何才知道前面有一面镜子或者一堵玻璃墙？它要如何知道前面的路面上有积水或者结冰了？</p>
<p>很有趣的事情，理解光线的反射和折射，似乎已经固化到了每个动物的视觉系统里面。我的猫能够从穿衣镜的镜像看见我在另外一间屋拿着逗猫绳，冲过来的时候却不会撞到镜子上面，到镜子面前立马掉一个头，冲向正确的方向。</p>
<p><img src="https://www.yinwang.org/images/monet-mirror.jpg" width="40%" /></p>
<p>深度神经网络从来没有考虑过这些问题，其中没有理解的成分存在，又何谈实现人类级别的视觉能力呢？如果观察的过程中缺少了理解，将会严重影响识别的准确性。</p>
<p>现在的深度学习模型都是基于像素的，没有抽象能力，不能构造 3D 拓扑模型，甚至连位置都分不清楚。缺乏人类视觉系统的这种“拓扑抽象理解”能力，可能就是为什么深度学习模型需要那么多的数据，那么多的计算，才勉强能得出物体的名字。而小孩子识别物体根本不需要那么多数据和计算，看一两次就知道这东西是什么了。</p>
<p>“神经网络”跟神经元的关系是很肤浅的。“神经网络”应该被叫做“可求导编程”，说白了就是利用微积分，用大量数据拟合出一个函数，所以它只能做拟合函数能做的那些事情。</p>
<p>用了千万张图片和几个星期的计算，拟合出来的函数也不是那么可靠。人们已经发现用一些办法生成奇怪的图片，能让最先进的深度神经网络输出<a href="http://www.evolvingai.org/fooling">完全错误的结果</a>。</p>
<p><img src="https://www.yinwang.org/images/diversity_40_images_label.png" width="90%" /></p>
<p>（图片来源：<a href="http://www.evolvingai.org/fooling">http://www.evolvingai.org/fooling</a>）</p>
<p>神经网络为什么会有这种缺陷呢？因为它只是拟合了一个粗略的“像素=&gt;名字”的函数。这函数碰巧能区分训练集里的图片，却不能抓住物体的结构和本质。它只是像素级别的拟合，所以这里面有很多空子可以钻。</p>
<p>研究表明，深度神经网络经常因为一些像素，颜色，纹理匹配了物体的一部分，就认为图片上有这个物体。神经网络无法像人类一样理解物体的结构和拓扑关系，所以才会被像素级别的肤浅假象所欺骗。</p>
<p>比如下面两个奇怪的图片，被认为是一个菠萝蜜和一个遥控器，只是因为它们中间出现了相似的纹理。</p>
<p><img src="https://www.yinwang.org/images/dnn-fool.jpg" width="70%" /></p>
<p>另外，神经网络还无法区分纹理的位置，所以它会把一些位置错乱的图片也识别成某种物体。比如下面这个，被认为是一张人脸，却无法发现五官都错位了。</p>
<p><img src="https://www.yinwang.org/images/strange-face.jpg" width="60%" /></p>
<p>神经网络为什么会犯这种愚蠢的错误呢？因为它的目标只是把训练集里的图片正确分类，提高“准确率”。至于怎么分类，它是毫无原则的，它完全不理解物体的结构。它并没有看到“叶子”，“果皮”，“方盒子”，“按钮”，它看到的只是一堆像素纹理。因为训练集里面的图片，出现了类似纹理的都被标记为了“菠萝蜜”和“遥控器”，没有出现这种纹理的都被标记为其它物品。所以神经网络找到了一个区分它们的“分界点”，认为看到这样的纹理，就一定是菠萝蜜和遥控器。傻不傻？</p>
<p>这就像应试教育训练出来的学生，他们的目标函数是“考高分”，为此他们不择手段。等毕业工作遇到现实的问题，他们就傻眼了，发现自己没学会什么东西。因为他们学习的时候只是在训练自己“从 ABCD 里区分出正确答案”。等到现实中没有 ABCD 的时候，他们就不知道怎么办了。</p>
<p>深度学习训练出来的那些“参数”是不可解释的，因为它们存在的目的只是把数据拟合出来，把不同种类的图片分离开，而没有任何意义。AI 人士喜欢给这种“不可解释性”找借口，甚至有人说：“神经网络学到的数据虽然不可解释，但它却出人意料的有效。这些学习得到的模型参数，其实就是知识！”</p>
<p>真的那么有效吗？那为什么能够被如此轻易的欺骗呢？这些“学习”得到的参数根本就不是本质的东西，不是知识，真的就是一堆毫无道理可言的数字，只为了降低“误差”，能够把特征空间的图片区分开来，所以神经网络才能被这样钻空子。</p>
<p>说这些参数是知识，就像在说考试猜答案的技巧是知识一样可笑。“因为已经连续十道题没有出现 A 了，所以这道题选 A”，或者“另外几套题的第十题都是 B，所以这套题的第十题也选 B”…… 深度学习拟合函数，就像拿历年高考题和它们的答案来拟合函数一样，想要不理解科目就做出答案来。有些时候它确实可以蒙对答案，但遇到前所未见的题目，或者题目被换了一下顺序，就傻眼了。</p>
<p>人为什么可以不受这种欺骗呢？因为人提取了高级的拓扑结构，不是瞎蒙的，所以人的判断不受像素的影响。因为提取了高级结构信息，人的观察是具有可解释性的。如果你问一个小孩，为什么你说这是一只猫而不是一只狗呢？她会告诉你：“因为它的耳朵是这样的，它的牙是那样的，它走路的姿势是那样的，它常常伸懒腰……”</p>
<p>做个实验好了，你可以问问你们家孩子，为什么他们认为这是一只猫而不是一只狗？</p>
<p><img src="https://www.yinwang.org/images/luoxiaohei.jpg" width="60%" /></p>
<p>神经网络看到一堆像素，很多层处理之后也不知道是什么结构，分不清“耳朵”和“嘴”，更不要说“走路”之类的动态概念了，所以它也就无法告诉你它认为这是猫的原因了。拟合的函数碰巧把这归成了猫，如果你要追究原因，很可能是肤浅的原因：图片上有一块像素匹配了图片库里某只猫的毛色纹理一类的。</p>
<p>所以人的视觉系统很可能是跟深度神经网络原理完全不同的，或者只有最低级的部分（比如线条检测）有相似之处。</p>
<p>请不要轻易地以为以上指出的问题有解决方案。我提到上面的问题时，有些人会跟我提 Hinton 的  <a href="https://en.wikipedia.org/wiki/Capsule_neural_network">capsule network</a>。很可惜 capsule network 的计算量要求异常的大，根本无法投入实用。从以上的分析你也许已经看出来，capsule network 根本没有抓住问题的关键，所以它不可能解决视觉理解的问题。</p>
<p>为什么 AI 人士总是认为视觉系统的高级功能都能通过“学习”得到呢？非常可能的事情是，这些强大的功能根本不是学来的，而是早就固化在基因里了。人和动物生下来就跟机器不一样，“硬件”在娘胎里就已经有了，只等发育和“激活”。纵使你有再多的数据，再多的计算力，你能超越为期几十亿年的，地球规模的自然进化和选择吗？</p>
<p>脑科学研究者很可能并没有完全搞明白神经元是如何工作的，神经元与神经网络的关系是肤浅的。每当你质疑神经网络与神经元的关系，AI 研究者就会抬出 Hubel &amp; Wiesel 在 1959 年拿猫做的那个<a href="http://youtube.com/watch?v=8VdFf3egwfg">实验</a>：“有人已经证明了视觉系统就是那样工作的！”</p>
<p>我问你啊，如果我们在 1959 年就已经知道动物视觉系统的实现细节，为什么现在还各种模型改来改去，训练来训练去呢？直接模仿过来不就行了？所以这些人的说法是自相矛盾的。</p>
<p>你想过没有，为什么到了 2019 年，人们还拿一个 60 年前的实验来说明问题？这 60 年来就没有新的发现了吗？而且从 H&amp;W 的实验你可以看出来，它只说明了猫的视觉神经有什么样的底层功能（能够检测“线条”），却没有证明那就是全部的构造。</p>
<p>H&amp;W 的实验只发现了最底层的“线检测”，却没有揭示这些底层神经元的信号到了上层是如何组合在一起的。“线检测”是图像处理的基础操作。一个能够识别拓扑结构的动物视觉系统，理所当然应该能做“线检测”，但它应该不止有这种低级功能。</p>
<p>视觉系统应该还有更高级的结构，H&amp;W 的实验并没能回答这个问题，它仍然是一个黑盒子。AI 研究者们却拿着 H&amp;W 的结果大做文章，自信满满的认为已经破解了动物视觉系统的一切奥秘。现在的深度神经网络基本是瞎蒙出来的。把一堆像素操作叠在一起，然后对大量数据进行“训练”，以为这样就能得到所有的视觉功能。</p>
<p>设计神经网络的所谓“算法工程师”，“数据科学家”，工作基本跟“炼丹术士”（alchemist）一样，拿个模型这改改那改改，拿海量的图片来训练，消耗大量的计算力和电能，“准确率”提高了，就发 paper。至于为什么效果会好一些，其中揭示了什么原理，模型里的操作是用来达到什么效果的，不知道。所谓“准确率”也不是通过大量实际未知数据算出来的，而是早就存在那里的测试数据，反反复复都是那些。</p>
<p>我很怀疑这样的研究方式能够带来什么质的突破，这不是科学的方法。如果你跟我一样，把神经网络看成是用“可求导编程语言”写出来的代码，那么现在这种设计模型的方法就很像“一百万只猴子敲键盘”，总有一只能敲出“Hello World！”</p>
<p>虽然 H&amp;W 对生理学做出了杰出的贡献，他们值得拿个诺贝尔奖，然而我们应该明白那只是一个开端，我们仍然处于理解视觉系统原理的初级阶段。拿一个初步的实验结果作为“终极答案”，号称“超越人类”，还想拿来做“自动驾驶车”，这不是 H&amp;W 的错，只能怪后人断章取义，过度自信了。</p>
<p>如果没有真正的视觉理解，依赖于图像识别技术的“自动驾驶车”，是不可能在复杂的情况下做出正确操作，保障人们安全的。机器人等一系列技术，也只能停留在固定场景，精确定位的“工业机器人”阶段，而不能在复杂的自然环境中行动。</p>
<p>我认识一些工业机器人的研究者。他们告诉我，深度神经网络那些识别算法太不精确了，根本没法用于准确性要求很高的应用。工业机器人控制不精确是完全不可接受的，所以他们都不用深度神经网络来控制机器人。</p>
<p>要实现真正的语言理解和视觉理解是非常困难的。真正的 AI 其实没有起步，AI 专家们忙着忽悠和布道，根本没人关心其中的本质，又何谈实现呢？我不是给大家泼凉水，初级的识别技术还是有用的，而且蛮有趣。但除非真正有人关心到问题所在，真的去研究本质的问题，否则实现 AI 就只是空中楼阁。我只是提醒大家不要盲目乐观，不要被忽悠了。</p>
</div>
    